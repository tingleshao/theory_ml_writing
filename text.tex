% ----------------------------------------------------------------
% Article Class (This is a LaTeX2e document)  ********************
% ----------------------------------------------------------------
\documentclass[12pt]{article}
\usepackage[english]{babel}
\usepackage{amsmath,amsthm}
\usepackage{amsfonts}
% THEOREMS -------------------------------------------------------
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\usepackage{amssymb}
\usepackage{setspace}
\numberwithin{equation}{section}
\doublespacing
% ----------------------------------------------------------------
\begin{document}


\title{The Derivation of Generalization Bounds for Distance Weighted Discrimination}%
\author{Chong Shao}%
%\address{}%
%\thanks{}%
%\date{}%
% ----------------------------------------------------------------

\maketitle
% ----------------------------------------------------------------
\section{Introduction}
Probably Approximate Correct (PAC) learning framework is a useful tool in examining the performance of a learning algorithm with respect to the sample, time and space complexity. It builds the theoretical foundation of the area of machine learning. Tools like Radmacher complexity and VC-dimension are used in order to present the generalization bound of a learning algorithm in some cases. In previous work, people have derived the generalization bound for Support Vector Machines (SVM). In this paper, the idea of PAC framework and the derivation of SVM's generalization bound are presented. The possibility of the derivation of a generalization bound for another learning algorithm which is designed for high dimensional, low sample size scenario: Distance Weighted Discrimination (DWD) is discussed.
\section{PAC Learning Model}
Probably Approximate Correct (PAC) learning framework was introduced by Valiant [1], to help define the class of learnable concepts in terms of sample complexity, time complexity and space complexity of a learning algorithm [2]. In the definition of PAC-learning, sample complexity is related to the number of training sample points needed to achieve an approximate solution.
\subsection{Definitions}
Several definitions need to be introduced before introducing the PAC learning model. \\[0.2cm]
1. $\mathcal{X}$: the set of all possible examples.\\[0.2cm]
2. $\mathcal{Y} = \{0,1\}$: set of all possible labels in binary classification. \\[0.2cm]
3. A \emph{concept} \(c: \mathcal{X} \rightarrow \mathcal{Y}\): a mapping from \(\mathcal{X}\) to \(\mathcal{Y}\). \\[0.2cm]
4. A \emph{concept class} $C$: a set of concepts.  \\[0.2cm]
5. Assume that examples are independently and identically distributed (i.i.d) according to some fixed distribution $D$. In most cases, for the learner (the learning algorithm), $D$ is unknown. \\[0.2cm]
6. In the case of supervised learning, the learning algorithm receives a set of training examples $S = (x_1, \dots , x_m)$ and the corresponding labels $C=(c(x_1), \dots, c(x_m))$. $c \in C$ is called the \emph{target concept} that we want to learn. \\[0.2cm]
7. The learning algorithm will use the training examples to produce a hypothesis $h_S \in H$ selected from a hypothesis set $H$ that maps $\mathcal{X}$ to $\mathcal{Y}$. Note that $H$ and $C$ may not be the same.\\[0.2cm]
8. It is considered good if a yielded hypothesis $h_S$ has a small generalization error. \\[0.2cm]
9. Definition of \textbf{Generalization error}:[2] \\[0.2cm]
Given a hypothesis $h \in H$, a target concept $c \in C$, and an underlying distribution $D$, the generalization error $R(h)$ is defined by
\[R(h)= \underset{x\sim D}{\text{Pr}}[h(x)\neq c(x)]= \underset{x \sim D}{\text{E}}[1_{h(x) \neq c(x)}]\]
The generalization error $R(h)$ cannot be directly computed since the concept $c$ is unknown. However the \emph{empirical error} of a hypothesis can be computed to estimate the generalization error. \\[0.2cm]
10. Definition of \textbf{Empirical error}: [2] \\[0.2cm]
Given a hypothesis $h \in H$, a target concept $c \in C$, and a sample $S = (x_1,\dots,x_m)$ with corresponding labels $C = (c(x_1),\dots,c(x_m))$, the generalization error $R(h)$ is defined by
\[\widehat{R}(h)= \frac{1}{m}\sum_{i=1}^{m}1_{h(x_i) \neq c(x_i)}\]
It can be proved that the expectation of empirical error based on an i.i.d. sample $S$ equals to the generalization error. [2]
\begin{align*}
 \underset{S \sim D^m}{\text{E}}[\widehat{R}(h)] &= \frac{1}{m}\sum_{i=1}^{m}\underset{S \sim D^m}{\text{E}}[1_{h(x_i) \neq c(x_i)}] \\
 &= \frac{1}{m}\sum_{i=1}^{m}\underset{S \sim D^m}{\text{E}}[1_{h(x) \neq c(x)}] \text{  (Because samples are drown i.i.d.)} \\
 &= \underset{S \sim D^m}{\text{E}}[1_{h(x) \neq c(x)}] \\
 &= \underset{x \sim D}{\text{E}}[1_{h(x) \neq c(x)}] \\
 &= R(h)
\end{align*}
\subsection{PAC Model}
A concept class $C$ is called PAC-learnable if the hypothesis returned by the algorithm after observing a number of points in polynomial of $1/\epsilon$ and $1/\delta$ has error at most $\epsilon$ with probability at least $1-\delta$. If $\epsilon$ and $\delta$ are small, we call the learning algorithm is approximately correct with high probability.[2] \\[0.2cm]
\textbf{Formal definition}:\\[0.2cm]
A concept class $C$ is said to be PAC-learnable if there exists an algorithm $\mathcal{A}$ and a polynomial function $p$ such that for any $\epsilon > 0$ and $\delta > 0$, for all distributions $D$ on $\mathcal{X}$ and for any target concept $c \in C$, for following inequality holds for any sample size $ m \geq p(1/\epsilon,1/\delta,n,size(c) )$:
\[\underset{S \sim D^m}{\text{Pr}}[R(h_S) \leq \epsilon ] \geq 1 - \delta \]
Note that PAC does not make assumption on the distribution D. For an algorithm, the concept class $C$ is determined (for example, all possible binary codes in a given length, or a straight line in 2D space) but a particular concept $c$ is unknown. \\[0.2cm]
(It can be shown that this inequality can be used to bound generalization error)
\subsection{Examples}
1. Finite $H$, consistent case \\[0.2cm]
Finite $H$ means that the number of possible returned hypothesis $h$ is finite. Consistent means the returned hypothesis makes no error on the training data sample $S$ [2].\\[0.2cm]
Let $H$ be a finite set of functions mapping from $\mathcal{X}$ to $\mathcal{Y}$. Let $\mathcal{A}$ be an algorithm that for any target concept $c \in H$ and \emph{i.i.d.} sample $S$ returns a consistent hypothesis $h_{S}$: $\widehat{R}(h_S) = 0$. Then, for any $\epsilon, \delta > 0$, the inequality $\text{Pr}_{S \sim D^m}[R(h_S) \leq \epsilon] \geq 1-\delta$ holds if
\[m \geq \frac{1}{\epsilon}(log|H|+log\frac{1}{\delta})\] \\[0.2cm]
Then we can get a inequality for $R(h)$
\[R(h_S) \leq \frac{1}{m}(log|H| + log\frac{1}{\delta})\]
\textbf{Proof:}\\[0.2cm]
We think that having a fixed $\epsilon$, the probability of having a consistent hypothesis $h_S$ that has error larger than $\epsilon$:
\[\text{Pr}[\exists h \in H: \widehat{R}(h) > 0  \wedge R(h) > \epsilon]\]
\[= \text{Pr}[(h_1 \in H, \widehat{R}(h_1)=0 \wedge R(h_1) \geq \epsilon) \vee (h_2 \in H, \widehat{R}(h_2) = 0 \wedge R(h_2) > \epsilon) \vee \dots]\]
\[\leq \sum_{h\in H}\text{Pr}[\widehat{R}(h) = 0 \wedge R(h)> \epsilon]\]
\[\leq \sum_{h\in H}\text{Pr}[\widehat{R}(h) = 0 | R(h)> \epsilon]\]
for any $h$ with $R(h) > \epsilon$, the probability having $\widehat{R}(h) = 0$ is bounded by:
\[\text{Pr}[\widehat{R}(h)=0| R(h)>\epsilon] \leq (1-\epsilon)^m\]
Apply this inequality to the previous inequality and consider the finite hypothesis family $H$:
\[\text{Pr}[\exists h \in H: \widehat{R}(h) = 0 \wedge R(h) > \epsilon] \leq |H|(1-\epsilon)^m \leq |H|e^{-m\epsilon}\]
\[\text{Pr}[R(h_S) \leq \epsilon] \geq 1 - |H|e^{-m\epsilon} \geq 1 - \delta  \]
\[\delta \geq |H|e^{-m\epsilon}\]
\[1 \geq \frac{|H|}{\delta}e^{-m\epsilon}\]
\[m\epsilon\ \geq log|H| + log\frac{1}{\delta}\]
\[m \geq \frac{1}{\epsilon} ( log|H| + log\frac{1}{\delta})\]
\[\text{Q.E.D.}\]\\[0.2cm]
This shows that if a concept is learnable, it is related to the size of hypothesis set, the degree of confidence the learner requires and the size of training examples. For this particular example, it shows that every consistent learning algorithm with finite size hypothesis set is PAC-learnable.\\[0.2cm]
2. Finite H, inconsistent case\\[0.2cm]
If it is in the case that there is no way to get a hypothesis with no error on the training data, it is called inconsistent case. For this situation, we want the actual error close to the empirical error. The inequality should contain the term $\widehat{R}(h)$. The inequality is called \textbf{generalization bound}.\\[0.2cm]
Hoeffding's inequality says:\\[0.2cm]
Let $X_1,\dots X_m$ be independent random variables with $X_i$ taking values in $[a_i,b_i]$ for all $i\in[1,m]$. Then for any $\epsilon > 0$, the following inequalities hold for $S_m = sum_{i=1}^m X_i:$
\[\text{Pr}[S_m-\text{E}[S_m] \geq \epsilon] \leq e^{-2\epsilon^2 / \sum_{i=1}^m(b_i-a_i)^2}\]
\[\text{Pr}[S_m-\text{E}[S_m] \leq -\epsilon] \leq e^{-2\epsilon^2 / \sum_{i=1}^m(b_i-a_i)^2}\]
This inequality can be used in this way:\\[0.2cm]
Fix $\epsilon >0$ and let $S$ denote an \emph{i.i.d.} sample of size $m$. For any hypothesis $h$, the following inequalities hold:
\[\text{Pr}_{S\sim D^m}[\widehat{R}(h) - R(h) \geq \epsilon] \leq e^{-2m\epsilon^2}\]
\[\text{Pr}_{S\sim D^m}[\widehat{R}(h) - R(h) \leq -\epsilon] \leq e^{-2m\epsilon^2}\]
Then we have
\[\text{Pr}_{S\sim D^m}[|\widehat{R}(h) - R(h)| \geq \epsilon] \leq 2 e^{-2m\epsilon^2}\]
Then the following generalization bound inequality holds:\\[0.2cm]
Let H be a finite hypothesis set, Then for any $\delta > 0$, with probability at least $1-\delta$, the following inequality holds:
\[\forall h \in H, R(h) \leq \widehat{R}(h) + \sqrt{\frac{log|H|+log\frac{2}{\delta}}{2m}}\]
\textbf{Proof:}
\[\text{Pr}[\exists h \in H | |\widehat{R}(h) - R(h)|>\epsilon] = \text{Pr}[(|\widehat{R}(h_1)-R(h_1)| > \epsilon)\vee \dots \vee (|\widehat{R}(h_m)-R(h_m)| > \epsilon)]\]
\[\leq \sum_{h\in H}\text{Pr}[|\widehat{R}(h)-R(h)|> \epsilon]\]
\[\leq 2|H|e^{-2m\epsilon^2}\]
\textbf{Link to the idea of Occam's razor principle}\\[0.2cm]
The basic idea of introducing Occam's razor principle is that, for two learning algorithms that performs equally well in terms of empirical error on a given data, the one with less complexity is preferred. For example, the one which generates a linear separating hyperplane is preferred in stead of the one which yields an nonlinear separating boundary. This can be shown in the next section, by examing the inequality (generalization bound) using Rademacher complexity, for a learning algorithm with function family with smaller Rademacher complexity, the bound is tighter.
\subsection{Rademacher Complexity}
Rademacher Complexity: \\[0.2cm]
In order to produce generalization bound for infinite hypothesis sets, (for example, lines on 2D plane), some tools are required. The first one is Rademacher Complexity. \\[0.2cm]
Definition of \textbf{Empirical Rademacher complexity}[2]: \\[0.2cm]
Let $G$ be a family of functions mapping from $Z$ to $[a,b]$ and $S=(z_1,\dots,z_m)$ a fixed sample of size $m$ with elements in $Z$. Let $g$ be a function in the function family $G$. And we write $\mathbf{g}_S = (g(z_1),\dots,g(z_m))^T$. The empirical Rademacher complexity of $G$ with respect to $S$ is defined as:
\[\mathfrak{\widehat{R}}_S(G)=\underset{\mathbf{\sigma}}{\text{E}}[\underset{g \in G}{\text{sup}}\frac{1}{m}\sum_{i=1}^{m}\sigma_ig(z_i)]=\underset{\mathbf{\sigma}}{\text{E}}[\underset{g \in G}{\text{sup}}\frac{\mathbf{\sigma}\cdot\mathbf{g}_S}{m}]\]
$\mathbf{\sigma} = (\sigma_1,\dots, \sigma_m)^T$. And $\sigma_i$'s are independent uniform random variables taking values $\{-1,+1\}$. Like tossing a fair coin $m$ times. \\[0.2cm]
The definition can be understood as a measure of how well the function class $G$ correlates with $\mathbf{\sigma}$ over the sample $S$. Since $\mathbf{\sigma}$ can be understood as noise, Empirical Rademacher complexity measures the richness of function class $G$. A richer or more complex function class $G$ can generate more $\mathbf{g}_S$ and will on average better correlate with random noise.\\[0.2cm]
Definition of \textbf{Rademacher Complexity}: \\[0.2cm]
For any integer $m\geq 1$, let $G$ be a family of functions mapping from $Z$ to $[a,b]$ and $S=(z_1,\dots,z_m)$ a fixed sample of size $m$ with elements in $Z$. The Rademacher complexity of $G$ is defined as:
\[\mathfrak{R}_m(G)=\underset{S \sim D^m}{\text{E}}[\mathfrak{\widehat{R}}_S(G)]\]
\textbf{Generalization bound for binary classification using Rademacher complexity}:\\[0.2cm]
Let $H$ be a family of functions taking values in $\{-1,+1\}$. Let $D$ be the distribution over the input space $\mathcal{X}$. For any $\delta > 0$, with probability at least $1-\delta$ over a sample $S$ of size $m$ drawn according to $D$, each of the following holds for any $h \in H$:
\[R(h)\leq \widehat{R}(h) + \mathfrak{R}_m(H) + \sqrt{\frac{\mathrm{log}\frac{1}{\delta}}{2m}}\]
\[R(h)\leq \widehat{R}(h) + \mathfrak{\widehat{R}}_S(H) + 3\sqrt{\frac{\mathrm{log}\frac{2}{\delta}}{2m}}\]
Proof: (later)
\subsection{Growth Function}
(Growth function is another measure of the complexity of a learning algorithm. This can be also explained for the sake of complete.)
\subsection{VC-dimension}
In many cases, obtaining $\mathfrak{\widehat{R}}_S(H)$ (Rademacher complexity) is computationally hard. Computing growth function is hard also. VC-dimension is introduced as an alternative. VC-dimension stands for Vapnik-Chervonenkis dimension.\\[0.2cm]
\textbf{Definition:}\\[0.2cm]
The VC-dimension of a hypothesis set $H$ is the size of the largest set that can be fully shattered by $H$:
\[VCdim(H) = max\{m:\Pi_H(m) = 2^m\}\]
\textbf{Example of shattering: Hyperplanes}\\[0.2cm]
\textbf{Generalization bound for binary classification using VC-Dimension}\\[0.2cm]
Let $H$ be a family of functions taking values in $\{-1,+1\}$ with VC-dimension d. Then for any $\delta > 0$, with probability at least $1-\delta$, the following holds for all $h \in H$:
\[R(h) \leq \widehat{R}(h) + \sqrt{\frac{2d\mathrm{log}\frac{em}{d}}{m}} + \sqrt{\frac{\mathrm{log}\frac{1}{\delta}}{2m}}\]
Proof: (later)
\section{Support Vector Machines}
Support Vector Machines (SVM) is a widely used tool for binary classification, invented by Vapnik. The SVM algorithm for general non-separable case was introduced by Cortes and Vapnik [3].
\subsection{Formulation}
For SVM algorithm, the hypothesis set is the set of hyperplanes.
\[H = \{x\mapsto \mathrm{sign}(\mathbf{w\cdot x}+b): \mathbf{w} \in \mathbb{R}^N, b \in \mathbb{R} \}\]
For a linearly separable data set, the goal is to find a hyperplane that successfully separate two group of points with labels $\{-1,+1\}$that, points with the same label will be on the same side of hte separating hyperplane. Here is the corresponding primal optimization problem:
\[\underset{\mathbf{w}, b}{\mathrm{min}} \frac{1}{2}\|\mathbf{w}\|^2\]
\[\text{subject to: } y_i(\mathbf{w\cdot x_i}+b) \geq 1, \forall i \in [1,m]\]
Note that in the problem we can scale $\mathbf{w}$ and $b$ to make $\underset{(x,y)\in S}{\mathrm{min}}|\mathbf{w\cdot x}+b| =1 $.
The objective function is quadratic and the constraint is affine. This problem belongs to quadratic programming (QP). It can be solved efficiently using many available optimization software. \\[0.2cm]
\textbf{Lagrangian and KKT condition and dual form: (later)} \\[0.2cm]
For this problem, we can write the Lagrangian:
\[\mathcal{L}(\mathbf{w},b,\mathbf{\alpha})=\frac{1}{2}\|\mathbf{w}\|^2-\sum_{i=1}^{m}\alpha_i [y_i(\mathbf{w\cdot x_i}+b)-1]\]
The dual form of the optimization problem shows that SVM only considers the support vectors when calculating the hyperplane therefore it needs to be shown.
\subsection{Leave-one-out analysis}
\textbf{Definition} \\[0.2cm]
Leave-one-out error can be understood as: having a set $S$ of training examples with size $m$, every time we take one training example $x_i$ out, train the classifier using the remaining data with the training algorithm $\mathcal{A}$, then use $x_i$ to test the SVM. We do it $m$ times and calculate error on average.
\[\widehat{R}_{LOO}(\mathcal{A})=\frac{1}{m}\sum_{i=1}^{m}1_{h_{S-\{x_i\}}(x_i)\neq y_i}\]
It can be proved that the expectation of leave-one-out error for training data size $m\geq 2$ equals to the expectation of generalization bound, for SVM with training data size $m-1$. This gives the information of the generalization bound of SVM algorithm in terms of average but not any suggestions on variance.
\[\underset{S\sim D^m}{\mathrm{E}}[\widehat{R}_{LOO}(\mathcal{A})]=\underset{S'\sim D^{m-1}}{\mathrm{E}}[R'(h_{S'})]\]
Proof:
\begin{align*}
\underset{S\sim D^m}{\mathrm{E}} [\widehat{R}_{LOO}(\mathcal{A})]  &= \frac{1}{m} \sum_{i=1}^m \underset{S \sim D^m}{\mathrm{E}}[1_{h_{S-\{x_i\}}(x_i)\neq y_i}] \\
                                                                   &= \underset{S \sim D^m}{\mathrm{E}}[1_{h_{S-\{x_i\}}(x_1)\neq y_1}] \\
                                                                   &= \underset{S' \sim D^{m-1}, x_1 \sim D}{\mathrm{E}}[1_{h_{S'}(x_1)\neq y_1}] \\
                                                                   &= \underset{S' \sim D^{m-1}}{\mathrm{E}}[\underset{x_1 \sim D}{\mathrm{E}}[1_{h_{S'}(x_1)\neq y_1}]] \\
                                                                   &= \underset{S' \sim D^{m-1}}{\mathrm{E}}[R(h_{S'})] \\
\end{align*}
\subsection{Soft Margin SVM Formulation}
(later)
\subsection{Margin Theory}
(later)
\subsection{Generalization Bound}
(later)
\section{Distance Weighted Discrimination}
Distance Weighted Discrimination (DWD) is another binary classification method introduced by Marron, Todd and Ahn [4]. DWD was designed for high dimension, low sample size (HDLSS) senecios, in which SVM method suffers from "data piling". \\[0.2cm]
In HDLSS context, data piling is a problem that, after projecting data points on the line normal to the separating hyperplane, the data points tends to group together at the regions close to the positive and negative margins. Data piling implies the loss of generalizability. From the learning bound point of view, the data piling effect means the number of support vectors is large, which implies the bound of the expectation of the generalization error is loose. \\[0.2cm]
DWD differs from SVM by taking into consideration all the data points in determining the separation hyperplane, rather than only considering points on the margin. Experiments done by Marron et.al. [4] shows that DWD can successfully avoid data piling. The projection of the data points on the direction perpendicular to the separation hyperplane are well separated.
\subsection{Formulation}
We consider the general case that the data points may not be linear separable. Like for the SVM, the hypothesis set, which is a set of separation hyperplane is:
\[H = \{x\mapsto \mathrm{sign}(\mathbf{w\cdot x}+b): \mathbf{w} \in \mathbb{R}^N, b \in \mathbb{R} \}\]
We define the residual $r_i$ of a data point $x_i$:
\[r_i = y_i(\mathbf{x_i'w}+b+\xi_i)\]
$\xi_i$ is a scalar for penalizing the misclassified points, like the general case SVM. When the data point is correctly classified and the penalization is not too small, $\xi_i = 0$. \\[0.2cm]
The optimization problem for DWD is:
\[\underset{\mathbf{r,w,}\beta,\mathbf{\xi}}{\mathrm{min}}\sum_i \frac{1}{r_i} + Ce'\xi\]
\[\text{subject to: }\mathbf{r} = \mathbf{YX'w} + \beta\mathbf{y} + \boldsymbol{\xi} \geq 0, \|\mathbf{w}\|^2 \leq 1, \xi \geq 0\]
Where $\mathbf{r}$ is the vector contains $r$ values. $\mathbf{Y}$ is $n$ by $n$ diagonal matrix with diagonal elements $y_i$ the labels of training data. $\mathbf{X}$ is the sample data matrix and $\boldsymbol{\xi}$ is the vector contains $\xi$ values.
\subsection{Weighted DWD}
(later) \\[0.2cm]
Weighted DWD [5] can be intuitively thought as a learning algorithm lying on the 'spectrum' between the original DWD and SVM. By specify the weight, the classifier can have the performance that considering the points away from the separating hyperplane much less important.
\section{References}
[1] Vapnik 1984 \\[0.2cm]
[2] Mohri et.al., \emph{``Foundations of Machine Learning"}, 2012 \\ [0.2cm]
[3] Cortes, Corinna; and Vapnik, Vladimir N.; \emph{``Support-Vector Networks"}, Machine Learning, 20, 1995.\\ http://www.springerlink.com/content/k238jx04hm87j80g/ \\[0.2cm]
[4] Merron, Todd and Ahn, DWD paper \\[0.2cm]
[5] Marron et.al., Weighted DWD paper
\end{document}
% ----------------------------------------------------------------
