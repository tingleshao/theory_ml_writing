% ----------------------------------------------------------------
% Article Class (This is a LaTeX2e document)  ********************
% ----------------------------------------------------------------
\documentclass[12pt]{article}
\usepackage[english]{babel}
\usepackage{amsmath,amsthm}
\usepackage{amsfonts}
% THEOREMS -------------------------------------------------------
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\usepackage{amssymb}
\numberwithin{equation}{section}
% ----------------------------------------------------------------
\begin{document}

\title{The Derivation of Generalization Bounds for Distance Weighted Discrimination}%
\author{Chong Shao}%
%\address{}%
%\thanks{}%
%\date{}%
% ----------------------------------------------------------------

\maketitle
% ----------------------------------------------------------------
\section{Introduction}
Probably Approximate Correct (PAC) learning framework is a useful tool in examining the performance of a learning algorithm with respect to the sample, time and space complexity. It builds the theoretical foundation of the area of machine learning. Tools like Radmacher complexity and VC-dimension are used in order to present the generalization bound of a learning algorithm in some cases. In previous work, people have derived the generalization bound for Support Vector Machines (SVM). In this paper, the idea of PAC framework and the derivation of SVM's generalization bound are presented. The possibility of the derivation of a generalization bound for another learning algorithm which is designed for high dimensional, low sample size scenario: Distance Weighted Discrimination (DWD) is discussed. 
\section{PAC Learning Model}
Probably Approximate Correct (PAC) learning framework was introduced by Valiant [1], to help define the class of learnable concepts in terms of sample complexity, time complexity and space complexity of a learning algorithm [2]. In the definition of PAC-learning, sample complexity is related to the number of training sample points needed to achieve an approximate solution.
\subsection{Definitions}
Several definitions need to be introduced before introducing the PAC learning model. \\[0.2cm]
1. $\mathcal{X}$: the set of all possible examples.\\[0.2cm]
2. $\mathcal{Y} = \{0,1\}$: set of all possible labels in binary classification. \\[0.2cm]
3. A \emph{concept} \(c: \mathcal{X} \rightarrow \mathcal{Y}\): a mapping from \(\mathcal{X}\) to \(\mathcal{Y}\). \\[0.2cm]
4. A \emph{concept class} $C$: a set of concepts.  \\[0.2cm]
5. Assume that examples are independently and identically distributed (i.i.d) according to some fixed distribution $D$. In most cases, for the learner (the learning algorithm), $D$ is unknown. \\[0.2cm]
6. In the case of supervised learning, the learning algorithm receives a set of training examples $S = (x_1, \dots , x_m)$ and the corresponding labels $C=(c(x_1), \dots, c(x_m))$. $c \in C$ is called the \emph{target concept} that we want to learn. \\[0.2cm]
7. The learning algorithm will use the training examples to produce a hypothesis $h_S \in H$ selected from a hypothesis set $H$ that maps $\mathcal{X}$ to $\mathcal{Y}$. Note that $H$ and $C$ may not be the same.\\[0.2cm]
8. It is considered good if a yielded hypothesis $h_S$ has a small generalization error. \\[0.2cm]
9. Definition of \textbf{Generalization error}:[2] \\[0.2cm]
Given a hypothesis $h \in H$, a target concept $c \in C$, and an underlying distribution $D$, the generalization error $R(h)$ is defined by
\[R(h)= \underset{x\sim D}{\text{Pr}}[h(x)\neq c(x)]= \underset{x \sim D}{\text{E}}[1_{h(x) \neq c(x)}]\]
The generalization error $R(h)$ cannot be directly computed since the concept $c$ is unknown. However the \emph{empirical error} of a hypothesis can be computed to estimate the generalization error. \\[0.2cm]
10. Definition of \textbf{Empirical error}: [2] \\[0.2cm]
Given a hypothesis $h \in H$, a target concept $c \in C$, and a sample $S = (x_1,\dots,x_m)$ with corresponding labels $C = (c(x_1),\dots,c(x_m))$, the generalization error $R(h)$ is defined by
\[\widehat{R}(h)= \frac{1}{m}\sum_{i=1}^{m}1_{h(x_i) \neq c(x_i)}\]
It can be proved that the expectation of empirical error based on an i.i.d. sample $S$ equals to the generalization error. [2]
\begin{align*}
 \underset{S \sim D^m}{\text{E}}[\widehat{R}(h)] &= \frac{1}{m}\sum_{i=1}^{m}\underset{S \sim D^m}{\text{E}}[1_{h(x_i) \neq c(x_i)}] \\
 &= \frac{1}{m}\sum_{i=1}^{m}\underset{S \sim D^m}{\text{E}}[1_{h(x) \neq c(x)}] \text{  (Because samples are drown i.i.d.)} \\
 &= \underset{S \sim D^m}{\text{E}}[1_{h(x) \neq c(x)}] \\
 &= \underset{x \sim D}{\text{E}}[1_{h(x) \neq c(x)}] \\
 &= R(h)
\end{align*}
\subsection{PAC Model}
A concept class $C$ is called PAC-learnable if the hypothesis returned by the algorithm after observing a number of points in polynomial of $1/\epsilon$ and $1/\delta$ has error at most $\epsilon$ with probability at least $1-\delta$. If $\epsilon$ and $\delta$ are small, we call the learning algorithm is approximately correct with high probability.[2] \\[0.2cm]
\textbf{Formal definition}:\\[0.2cm]
A concept class $C$ is said to be PAC-learnable if there exists an algorithm $\mathcal{A}$ and a polynomial function $p$ such that for any $\epsilon > 0$ and $\delta > 0$, for all distributions $D$ on $\mathcal{X}$ and for any target concept $c \in C$, for following inequality holds for any sample size $ m \geq p(1/\epsilon,1/\delta,n,size(c) )$:
\[\underset{S \sim D^m}{\text{Pr}}[R(h_S) \leq \epsilon ] \geq 1 - \delta \]
Note that PAC does not make assumption on the distribution D. For an algorithm, the concept class $C$ is determined (for example, all possible binary codes in give length, or a straight line on 2D plane), but a particular concept $c$ is unknown. \\[0.2cm]
(It can be shown that this inequality can be used to bound generalization error)
\subsection{Example (later)}
1. finite H, consistent case \\[0.2cm]
can get a inequality for $R(h)$\\[0.2cm]
2. finite H, inconsistent case \\[0.2cm]
can get a inequality for $R(h)$ \\[0.2cm]
Link to the idea of Occam's razor principle (later)
\subsection{Rademacher Complexity}
Rademacher Complexity: \\[0.2cm]
In order to produce generalization bound for infinite hypothesis sets, (for example, lines in 2D plane), some tools are required. One is Rademacher Complexity. \\[0.2cm]
Definition of \textbf{Empirical Rademacher complexity}: \\[0.2cm]
Let $G$ be a family of functions mapping from $Z$ to $[a,b]$ and $S=(z_1,\dots,z_m)$ a fixed sample of size $m$ with elements in $Z$. And we write $\mathbf{g}_S = (g(z_1),\dots,g(z_m))^T$. The empirical Rademacher complexity of $G$ with respect to $S$ is defined as:
\[\mathfrak{\widehat{R}}_S(G)=\underset{\mathbf{\sigma}}{\text{E}}[\underset{g \in G}{\text{sup}}\frac{1}{m}\sum_{i=1}^{m}\sigma_ig(z_i)]=\underset{\mathbf{\sigma}}{\text{E}}[\underset{g \in G}{\text{sup}}\frac{\mathbf{\sigma}\cdot\mathbf{g}_S}{m}]\]
$\mathbf{\sigma} = (\sigma_1,\dots, \sigma_m)^T$. And $\sigma_i$'s are independent uniform random variables taking values $\{-1,+1\}$. Like tossing a fair coin m times. \\[0.2cm]
The definition can be understood as a measure of how well the function class $G$ correlates with $\mathbf{\sigma}$ over the sample $S$. Since $\mathbf{\sigma}$ can be understood as noise, Empirical Rademacher complexity measures the richness of function class $G$. A richer or more complex function class $G$ can generate more $\mathbf{g}_S$ and will on average better correlate with random noise.\\[0.2cm]
Definition of \textbf{Rademacher Complexity}: \\[0.2cm]
For any integer $m\geq 1$, let $G$ be a family of functions mapping from $Z$ to $[a,b]$ and $S=(z_1,\dots,z_m)$ a fixed sample of size $m$ with elements in $Z$. The Rademacher complexity of $G$ is defined as:
\[\mathfrak{R}_m(G)=\underset{S \sim D^m}{\text{E}}[\mathfrak{\widehat{R}}_S(G)]\]
\textbf{Generalization bound for binary classification using Rademacher complexity}:\\[0.2cm]
Let $H$ be a family of functions taking values in $\{-1,+1\}$. Let $D$ be the distribution over the input space $\mathcal{X}$. For any $\delta > 0$, with probability at least $1-\delta$ over a sample $S$ of size $m$ drawn according to $D$, each of the following holds for any $h \in H$:
\[R(h)\leq \widehat{R}(h) + \mathfrak{R}_m(H) + \sqrt{\frac{\mathrm{log}\frac{1}{\delta}}{2m}}\]
\[R(h)\leq \widehat{R}(h) + \mathfrak{\widehat{R}}_S(H) + 3\sqrt{\frac{\mathrm{log}\frac{2}{\delta}}{2m}}\]
Proof: (later)
\subsection{VC-Dimension}
In some cases, computing $\mathfrak{\widehat{R}}_S(H)$ is computationally hard. VC-Dimension is introduced as an alternative. \\[0.2cm]
\textbf{Definition}
The VC-dimension of a hypothesis set $H$ is the size of the largest set that can be fully shattered by $H$:
\[VCdim(H) = max\{m:\Pi_H(m) = 2^m\}\]
\textbf{Example: Hyperplanes}
\textbf{Generalization bound for binary classification using VC-Dimension}
Let $H$ be a family of functions taking values in $\{-1,+1\}$ with VC-dimension d. Then for any $\delta > 0$, with probability at least $1-\delta$, the following holds for all $h \in H$:
\[R(h) \leq \widehat{R}(h) + \sqrt{\frac{2d\mathrm{log}\frac{em}{d}}{m}} + \sqrt{\frac{\mathrm{log}\frac{1}{\delta}}{2m}}\]
Proof: (later)
\section{Support Vector Machines}
Support Vector Machines (SVM) is a widely used tool for binary classification, invented by Vapnik. The SVM algorithm for general non-separable case was introduced by Cortes and Vapnik [3].
\subsection{Formulation}
For SVM algorithm, the hypothesis set is the set of hyperplanes.
\[H = \{x\mapsto \mathrm{sign}(\mathbf{w\cdot x}+b): \mathbf{w} \in \mathbb{R}^N, b \in \mathbb{R} \}\]
For a linearly separable data set, the goal is to find a hyperplane that successfully separate two group of points with labels $\{-1,+1\}$that, points with the same label will be on the same side of hte separating hyperplane. Here is the corresponding primal optimization problem:
\[\underset{\mathbf{w}, b}{\mathrm{min}} \frac{1}{2}\|\mathbf{w}\|^2\]
\[\text{subject to: } y_i(\mathbf{w\cdot x_i}+b) \geq 1, \forall i \in [1,m]\]
Note that in the problem we can scale $\mathbf{w}$ and $b$ to make $\underset{(x,y)\in S}{\mathrm{min}}|\mathbf{w\cdot x}+b| =1 $.
The objective function is quadratic and the constraint is affine. This problem belongs to quadratic programming (QP). \\[0.2cm]
\textbf{Lagrangian and KKT condition and dual form: (later)} \\[0.2cm]
For this problem, we can write the Lagrangian:
\[\mathcal{L}(\mathbf{w},b,\mathbf{\alpha})=\frac{1}{2}\|\mathbf{w}\|^2-\sum_{i=1}^{m}\alpha_i [y_i(\mathbf{w\cdot x_i}+b)-1]\]
The dual form of the optimization problem shows that SVM only considers the support vectors when calculating the hyperplane therefore it needs to be shown.
\subsection{Leave-one-out analysis}
\textbf{Definition} \\[0.2cm]
Leave-one-out error can be understood as: having a set $S$ of training examples with size $m$, every time we take one training example $x_i$ out, train the classifier using the remaining data with the training algorithm $\mathcal{A}$, then use $x_i$ to test the SVM. We do it $m$ times and calculate error on average.
\[\widehat{R}_{LOO}(\mathcal{A})=\frac{1}{m}\sum_{i=1}^{m}1_{h_{S-\{x_i\}}(x_i)\neq y_i}\]
It can be proved that the expectation of leave-one-out error for training data size $m\geq 2$ equals to the expectation of generalization bound, for SVM with training data size $m-1$. This gives the information of the generalization bound of SVM algorithm in terms of average but not any suggestions on variance.
\[\underset{S\sim D^m}{\mathrm{E}}[\widehat{R}_{LOO}(\mathcal{A})]=\underset{S'\sim D^{m-1}}{\mathrm{E}}[R'(h_{S'})]\]
Proof:
\begin{align*}
\underset{S\sim D^m}{\mathrm{E}} [\widehat{R}_{LOO}(\mathcal{A})]  &= \frac{1}{m} \sum_{i=1}^m \underset{S \sim D^m}{\mathrm{E}}[1_{h_{S-\{x_i\}}(x_i)\neq y_i}] \\
                                                                   &= \underset{S \sim D^m}{\mathrm{E}}[1_{h_{S-\{x_i\}}(x_1)\neq y_1}] \\
                                                                   &= \underset{S' \sim D^{m-1}, x_1 \sim D}{\mathrm{E}}[1_{h_{S'}(x_1)\neq y_1}] \\
                                                                   &= \underset{S' \sim D^{m-1}}{\mathrm{E}}[\underset{x_1 \sim D}{\mathrm{E}}[1_{h_{S'}(x_1)\neq y_1}]] \\
                                                                   &= \underset{S' \sim D^{m-1}}{\mathrm{E}}[R(h_{S'})] \\
\end{align*}
\subsection{Soft Margin SVM Formulation}
(later)
\subsection{Margin Theory}
(later)
\subsection{Generalization Bound}
\section{Distance Weighted Discrimination}
Distance Weighted Discrimination (DWD) is another binary classification method introduced by Merron, Todd and Ahn [4]. DWD was designed for high dimension, low sample size (HDLSS) senecios, in which SVM method suffers from "data piling". \\[0.2cm]
In HDLSS context, data piling is a problem that, after projecting data points on the line normal to the separating hyperplane, the data points tends to group together at the regions close to the positive and negative margins. Data piling implies the loss of generalizability. From the learning bound point of view, the data piling effect means the number of support vectors is large, which implies the bound of the expectation of the generalization error is loose. \\[0.2cm]
DWD differs from SVM by taking into consideration all the data points in determining the separation hyperplane, rather than only considering points on the margin. Experiments done by Marron et.al. [4] shows that DWD can successfully avoid data piling. The projection of the data points on the direction perpendicular to the separation hyperplane are well separated.
\subsection{Formulation}
We consider the general case that the data points may not be linear separable. Like for the SVM, the hypothesis set, which is a set of separation hyperplane is:
\[H = \{x\mapsto \mathrm{sign}(\mathbf{w\cdot x}+b): \mathbf{w} \in \mathbb{R}^N, b \in \mathbb{R} \}\]
We define the residual $r_i$ of a data point $x_i$:
\[r_i = y_i(\mathbf{x_i'w}+b+\xi_i)\]
$\xi_i$ is a scalar for penalizing the misclassified points, like the general case SVM. When the data point is correctly classified and the penalization is not too small, $\xi_i = 0$. \\[0.2cm]
The optimization problem for DWD is:
\[\underset{\mathbf{r,w,}\beta,\mathbf{\xi}}{\mathrm{min}}\sum_i \frac{1}{r_i} + Ce'\xi\]
\[\text{subject to: }\mathbf{r} = \mathbf{YX'w} + \beta\mathbf{y} + \boldsymbol{\xi} \geq 0, \|\mathbf{w}\|^2 \leq 1, \xi \geq 0\]
Where $\mathbf{r}$ is the vector contains $r$ values. $\mathbf{Y}$ is $n$ by $n$ diagonal matrix with diagonal elements $y_i$ the labels of training data. $\mathbf{X}$ is the sample data matrix and $\boldsymbol{\xi}$ is the vector contains $\xi$ values.
\subsection{Weighted DWD}

\section{References}
[1] Vapnik 1984 \\[0.2cm]
[2] Mohri et.al. \emph{``Foundations of Machine Learning"}, 2012 \\ [0.2cm]
[3] Cortes, Corinna; and Vapnik, Vladimir N.; \emph{``Support-Vector Networks"}, Machine Learning, 20, 1995.\\ http://www.springerlink.com/content/k238jx04hm87j80g/ \\[0.2cm]
[4] Merron, Todd and Ahn, DWD paper
\end{document}
% ----------------------------------------------------------------
